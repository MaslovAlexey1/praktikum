{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"blue\">Привет. Давай смотреть как ты укротил тексты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "\n",
    "Обучите модель классифицировать комментарии на позитивные и негативные. В вашем распоряжении набор данных с разметкой о токсичности правок.\n",
    "\n",
    "Постройте модель со значением метрики качества *F1* не меньше 0.75. \n",
    "\n",
    "### Инструкция по выполнению проекта\n",
    "\n",
    "1. Загрузите и подготовьте данные.\n",
    "2. Обучите разные модели. \n",
    "3. Сделайте выводы.\n",
    "\n",
    "Для выполнения проекта применять *BERT* необязательно, но вы можете попробовать.\n",
    "\n",
    "### Описание данных\n",
    "\n",
    "Данные находятся в файле `toxic_comments.csv`. Столбец *text* в нём содержит текст комментария, а *toxic* — целевой признак."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Подготовка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from pathlib import Path\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils import resample\n",
    "import re\n",
    "import nltk\n",
    "from pathlib import Path\n",
    "path = Path.cwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "[nltk_data] Downloading package wordnet to\n[nltk_data]     /Users/alexeymaslov/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package punkt to\n[nltk_data]     /Users/alexeymaslov/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /Users/alexeymaslov/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preparing():\n",
    "    path = Path().absolute()\n",
    "    df = pd.read_csv('{}/datasets/toxic_comments.csv'.format(path))\n",
    "    return df\n",
    "\n",
    "def train_valid_test_split(df):\n",
    "    features_train, features_test, target_train, target_test = train_test_split(df['lemm_text'], df['toxic'], test_size = 0.2, random_state=12345)\n",
    "    features_train, features_valid, target_train, target_valid = train_test_split(features_train, target_train, test_size = 0.25, random_state=12345)\n",
    "    return features_train, features_valid, features_test, target_train, target_valid, target_test\n",
    "\n",
    "def train_valid_test_split_df(df):\n",
    "    df_train, df_test = train_test_split(df, test_size = 0.2, random_state=12345)\n",
    "    df_train, df_valid = train_test_split(df_train, test_size = 0.25, random_state=12345)\n",
    "    return df_train, df_valid, df_test\n",
    "\n",
    "def features_target_split(df):\n",
    "    features = df['lemm_text']\n",
    "    target = df['toxic']\n",
    "    return features, target\n",
    "\n",
    "def lemmatize(m, text):\n",
    "    text = clear_text(text)\n",
    "    token_words=word_tokenize(text)\n",
    "    lemm_text = []\n",
    "    for word in token_words:\n",
    "        lemm_text.append(m.lemmatize(word, pos='v'))\n",
    "    return \" \".join(lemm_text)\n",
    "\n",
    "def clear_text(text):\n",
    "    text = re.sub(r'[^a-zA-Z ]', ' ', text)\n",
    "    return \" \".join(text.split())\n",
    "\n",
    "def downsample(df, ratio):\n",
    "    df_toxic = df[df['toxic'] == 1]\n",
    "    df_not_toxic = df[df['toxic'] == 0]\n",
    "    sample_size = int(df_not_toxic.shape[0] * ratio)\n",
    "    df_not_toxic = df_not_toxic.sample(sample_size, random_state=12345)\n",
    "    df_downsampled = pd.concat([df_toxic, df_not_toxic]) \n",
    "    return df_downsampled\n",
    "\n",
    "def upsample(df):\n",
    "    df_toxic = df[df['toxic'] == 1]\n",
    "    df_not_toxic = df[df['toxic'] == 0]\n",
    "    df_toxic = resample(df_toxic, replace=True, n_samples=df_not_toxic.shape[0], random_state=12345)\n",
    "    df_upsampled = pd.concat([df_toxic, df_not_toxic]).sample(frac=1, random_state=12345).reset_index(drop=True)\n",
    "    return df_upsampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лемматизируем текст"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = preparing()\n",
    "m = WordNetLemmatizer()\n",
    "df['lemm_text'] = df['text'].apply(lambda x: lemmatize(m, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_valid, df_test = train_valid_test_split_df(df)\n",
    "features_train, target_train =  features_target_split(df_train)\n",
    "features_valid, target_valid = features_target_split(df_valid)\n",
    "features_test, target_test = features_target_split(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 159571 entries, 0 to 159570\nData columns (total 4 columns):\n #   Column      Non-Null Count   Dtype \n---  ------      --------------   ----- \n 0   Unnamed: 0  159571 non-null  int64 \n 1   text        159571 non-null  object\n 2   toxic       159571 non-null  int64 \n 3   lemm_text   159571 non-null  object\ndtypes: int64(2), object(2)\nmemory usage: 4.9+ MB\nNone\n         Unnamed: 0          toxic\ncount  159571.00000  159571.000000\nmean    79785.00000       0.101679\nstd     46064.32424       0.302226\nmin         0.00000       0.000000\n25%     39892.50000       0.000000\n50%     79785.00000       0.000000\n75%    119677.50000       0.000000\nmax    159570.00000       1.000000\n0    0.898321\n1    0.101679\nName: toxic, dtype: float64\n(95742,)\n(31914,)\n(31915,)\n"
    }
   ],
   "source": [
    "print(df.info())\n",
    "print(df.describe())\n",
    "print(df.toxic.value_counts(normalize=True))\n",
    "\n",
    "print(features_train.shape)\n",
    "print(features_valid.shape)\n",
    "print(features_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Всего 160К комментариев, 10% из них токсичные. Пропусков в данных нет"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"blue\">Отлично"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем обучить линейную модель с помощью TF-IDF<br>\n",
    "Будем использовать только LogisticRegression модель. Посмотрим как будет меняться F1  в зависимости от параметров векторизации и настроек модели "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_valid, df_test = train_valid_test_split_df(df)\n",
    "features_train, target_train =  features_target_split(df_train)\n",
    "features_valid, target_valid = features_target_split(df_valid)\n",
    "features_test, target_test = features_target_split(df_test)\n",
    "corpus_train = features_train.values\n",
    "corpus_valid = features_valid.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Без стоп-слов; ngram из одного слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_tf_idf = TfidfVectorizer()\n",
    "count_tf_idf.fit(corpus_train)\n",
    "tf_idf_train = count_tf_idf.transform(corpus_train)\n",
    "tf_idf_valid = count_tf_idf.transform(corpus_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['aa', 'aaa', 'aaaa', 'aaaaa', 'aaaaaaaa']"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tf_idf.get_feature_names()[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Действительно, фичи из одного слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "F1-score: 0.738\n"
    }
   ],
   "source": [
    "lr_model = LogisticRegression(solver='liblinear')\n",
    "lr_model.fit(tf_idf_train, target_train)\n",
    "predictions = lr_model.predict(tf_idf_valid)\n",
    "print('F1-score: {:.3f}'.format(f1_score(target_valid, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "F1-score: 0.750\n"
    }
   ],
   "source": [
    "lr_model = LogisticRegression(solver='liblinear', class_weight='balanced')\n",
    "lr_model.fit(tf_idf_train, target_train)\n",
    "predictions = lr_model.predict(tf_idf_valid)\n",
    "print('F1-score: {:.3f}'.format(f1_score(target_valid, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "F1-score: 0.764\n"
    }
   ],
   "source": [
    "lr_model = LogisticRegression(solver='saga', penalty='elasticnet', l1_ratio=0.5)\n",
    "lr_model.fit(tf_idf_train, target_train)\n",
    "predictions = lr_model.predict(tf_idf_valid)\n",
    "print('F1-score: {:.3f}'.format(f1_score(target_valid, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "F1-score: 0.755\n"
    }
   ],
   "source": [
    "lr_model = LogisticRegression(solver='saga', penalty='elasticnet', l1_ratio=0.3)\n",
    "lr_model.fit(tf_idf_train, target_train)\n",
    "predictions = lr_model.predict(tf_idf_valid)\n",
    "print('F1-score: {:.3f}'.format(f1_score(target_valid, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "F1-score: 0.787\n"
    }
   ],
   "source": [
    "lr_model = LogisticRegression(solver='saga', penalty='l1')\n",
    "lr_model.fit(tf_idf_train, target_train)\n",
    "predictions = lr_model.predict(tf_idf_valid)\n",
    "print('F1-score: {:.3f}'.format(f1_score(target_valid, predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем поменять порог"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[[9.96979867e-01 3.02013315e-03]\n [8.35604203e-01 1.64395797e-01]\n [2.01393766e-04 9.99798606e-01]\n ...\n [9.98283139e-01 1.71686087e-03]\n [9.89344370e-01 1.06556297e-02]\n [7.42140097e-01 2.57859903e-01]]\n"
    }
   ],
   "source": [
    "probabilities = lr_model.predict_proba(tf_idf_valid)\n",
    "print(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Порог = 0.00 | F1-score = 0.188\nПорог = 0.02 | F1-score = 0.393\nПорог = 0.04 | F1-score = 0.542\nПорог = 0.06 | F1-score = 0.641\nПорог = 0.08 | F1-score = 0.690\nПорог = 0.10 | F1-score = 0.723\nПорог = 0.12 | F1-score = 0.742\nПорог = 0.14 | F1-score = 0.759\nПорог = 0.16 | F1-score = 0.768\nПорог = 0.18 | F1-score = 0.779\nПорог = 0.20 | F1-score = 0.783\nПорог = 0.22 | F1-score = 0.786\nПорог = 0.24 | F1-score = 0.789\nПорог = 0.26 | F1-score = 0.794\nПорог = 0.28 | F1-score = 0.794\nПорог = 0.30 | F1-score = 0.795\nПорог = 0.32 | F1-score = 0.794\nПорог = 0.34 | F1-score = 0.793\nПорог = 0.36 | F1-score = 0.794\nПорог = 0.38 | F1-score = 0.794\nПорог = 0.40 | F1-score = 0.794\nПорог = 0.42 | F1-score = 0.791\nПорог = 0.44 | F1-score = 0.791\nПорог = 0.46 | F1-score = 0.789\nПорог = 0.48 | F1-score = 0.787\n"
    }
   ],
   "source": [
    "probabilities_one = probabilities[:, 1]\n",
    "for threshold in np.arange(0, 0.5, 0.02):\n",
    "    predicted_valid = probabilities_one > threshold\n",
    "    f1score = f1_score(target_valid, predicted_valid)\n",
    "\n",
    "    print(\"Порог = {:.2f} | F1-score = {:.3f}\".format(\n",
    "        threshold, f1score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Лучший результат без стоп-слов F1-score: 0.795"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Попробуем подключить словарь стоп-слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Количество стоп-слов в словаре: 179\n"
    }
   ],
   "source": [
    "print('Количество стоп-слов в словаре: {}'.format(len(stop_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_tf_idf = TfidfVectorizer(stop_words=stop_words)\n",
    "count_tf_idf.fit(corpus_train)\n",
    "tf_idf_train = count_tf_idf.transform(corpus_train)\n",
    "tf_idf_valid = count_tf_idf.transform(corpus_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "F1-score: 0.776\n"
    }
   ],
   "source": [
    "lr_model = LogisticRegression(solver='saga', penalty='l1')\n",
    "lr_model.fit(tf_idf_train, target_train)\n",
    "predictions = lr_model.predict(tf_idf_valid)\n",
    "print('F1-score: {:.3f}'.format(f1_score(target_valid, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Порог = 0.00 | F1-score = 0.188\nПорог = 0.02 | F1-score = 0.359\nПорог = 0.04 | F1-score = 0.506\nПорог = 0.06 | F1-score = 0.633\nПорог = 0.08 | F1-score = 0.688\nПорог = 0.10 | F1-score = 0.719\nПорог = 0.12 | F1-score = 0.740\nПорог = 0.14 | F1-score = 0.754\nПорог = 0.16 | F1-score = 0.767\nПорог = 0.18 | F1-score = 0.775\nПорог = 0.20 | F1-score = 0.779\nПорог = 0.22 | F1-score = 0.784\nПорог = 0.24 | F1-score = 0.789\nПорог = 0.26 | F1-score = 0.791\nПорог = 0.28 | F1-score = 0.789\nПорог = 0.30 | F1-score = 0.789\nПорог = 0.32 | F1-score = 0.791\nПорог = 0.34 | F1-score = 0.789\nПорог = 0.36 | F1-score = 0.788\nПорог = 0.38 | F1-score = 0.788\nПорог = 0.40 | F1-score = 0.787\nПорог = 0.42 | F1-score = 0.785\nПорог = 0.44 | F1-score = 0.782\nПорог = 0.46 | F1-score = 0.780\nПорог = 0.48 | F1-score = 0.779\n"
    }
   ],
   "source": [
    "probabilities = lr_model.predict_proba(tf_idf_valid)\n",
    "probabilities_one = probabilities[:, 1]\n",
    "for threshold in np.arange(0, 0.5, 0.02):\n",
    "    predicted_valid = probabilities_one > threshold\n",
    "    f1score = f1_score(target_valid, predicted_valid)\n",
    "    print(\"Порог = {:.2f} | F1-score = {:.3f}\".format(\n",
    "        threshold, f1score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Стоп-слова никак не помогли"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При векторизации по дефолту все слова переводятся в нижний регистр. Предположим что в токсичных сообщениях больше слов с верхнем регистре, попробуем это учесть и не будем переводить слова в нижний регистр"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_tf_idf = TfidfVectorizer(lowercase=False)\n",
    "count_tf_idf.fit(corpus_train)\n",
    "tf_idf_train = count_tf_idf.transform(corpus_train)\n",
    "tf_idf_valid = count_tf_idf.transform(corpus_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "F1-score: 0.777\n"
    }
   ],
   "source": [
    "lr_model = LogisticRegression(solver='saga', penalty='l1')\n",
    "lr_model.fit(tf_idf_train, target_train)\n",
    "predictions = lr_model.predict(tf_idf_valid)\n",
    "print('F1-score: {:.3f}'.format(f1_score(target_valid, predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отмена lowercase тоже не помогла<br>\n",
    "#### Попробуем добавить фичи из двух слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_tf_idf = TfidfVectorizer(ngram_range=(1, 2))\n",
    "count_tf_idf.fit(corpus_train)\n",
    "tf_idf_train = count_tf_idf.transform(corpus_train)\n",
    "tf_idf_valid = count_tf_idf.transform(corpus_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['aa', 'aa aa', 'aa aat', 'aa acupuncture', 'aa again']"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tf_idf.get_feature_names()[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Действительно, появились фичи(основанные на них вектора) из двух слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "F1-score: 0.786\n"
    }
   ],
   "source": [
    "lr_model = LogisticRegression(solver='saga', penalty='l1')\n",
    "lr_model.fit(tf_idf_train, target_train)\n",
    "predictions = lr_model.predict(tf_idf_valid)\n",
    "print('F1-score: {:.3f}'.format(f1_score(target_valid, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Порог = 0.00 | F1-score = 0.188\nПорог = 0.02 | F1-score = 0.367\nПорог = 0.04 | F1-score = 0.504\nПорог = 0.06 | F1-score = 0.611\nПорог = 0.08 | F1-score = 0.668\nПорог = 0.10 | F1-score = 0.704\nПорог = 0.12 | F1-score = 0.729\nПорог = 0.14 | F1-score = 0.750\nПорог = 0.16 | F1-score = 0.761\nПорог = 0.18 | F1-score = 0.769\nПорог = 0.20 | F1-score = 0.776\nПорог = 0.22 | F1-score = 0.781\nПорог = 0.24 | F1-score = 0.785\nПорог = 0.26 | F1-score = 0.788\nПорог = 0.28 | F1-score = 0.789\nПорог = 0.30 | F1-score = 0.791\nПорог = 0.32 | F1-score = 0.792\nПорог = 0.34 | F1-score = 0.794\nПорог = 0.36 | F1-score = 0.795\nПорог = 0.38 | F1-score = 0.793\nПорог = 0.40 | F1-score = 0.793\nПорог = 0.42 | F1-score = 0.791\nПорог = 0.44 | F1-score = 0.790\nПорог = 0.46 | F1-score = 0.789\nПорог = 0.48 | F1-score = 0.787\n"
    }
   ],
   "source": [
    "probabilities = lr_model.predict_proba(tf_idf_valid)\n",
    "probabilities_one = probabilities[:, 1]\n",
    "for threshold in np.arange(0, 0.5, 0.02):\n",
    "    predicted_valid = probabilities_one > threshold\n",
    "    f1score = f1_score(target_valid, predicted_valid)\n",
    "    print(\"Порог = {:.2f} | F1-score = {:.3f}\".format(\n",
    "        threshold, f1score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Лучший результат получился для для логистической регрессии solver='saga', penalty='l1', threshold=0.3. Проверим модель на тесте"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=12345)\n",
    "features_train, target_train =  features_target_split(df_train)\n",
    "features_test, target_test = features_target_split(df_test)\n",
    "corpus_train = features_train.values\n",
    "corpus_test = features_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_tf_idf = TfidfVectorizer()\n",
    "count_tf_idf.fit(corpus_train)\n",
    "tf_idf_train = count_tf_idf.transform(corpus_train)\n",
    "tf_idf_test = count_tf_idf.transform(corpus_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "F1-score: 0.799\n"
    }
   ],
   "source": [
    "lr_model = LogisticRegression(solver='saga', penalty='l1')\n",
    "lr_model.fit(tf_idf_train, target_train)\n",
    "\n",
    "probabilities = lr_model.predict_proba(tf_idf_test)\n",
    "probabilities_one = probabilities[:, 1]\n",
    "predictions = probabilities_one > 0.3\n",
    "\n",
    "print('F1-score: {:.3f}'.format(f1_score(target_test, predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"blue\">Прям отлично и лайк лайк"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Удалось достичь F1-score = 0.799 даже без подключения BERT, т.е. просто по наличию конкретных слов в сообщении, без смыслового анализа\n",
    "2. Подключение стоп слов не всегда помогает\n",
    "3. В данном упражнении добавление би-грам никак не помогло. При этом словарь сильно увеличился(примерно в 10 раз)\n",
    "4. У модели логистической регрессии куча настроек, с ними можно играться\n",
    "5. Анализ текстов требует много ресурсов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Чек-лист проверки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [x]  Jupyter Notebook открыт\n",
    "- [x]  Весь код выполняется без ошибок\n",
    "- [x]  Ячейки с кодом расположены в порядке исполнения\n",
    "- [x]  Данные загружены и подготовлены\n",
    "- [x]  Модели обучены\n",
    "- [x]  Значение метрики *F1* не меньше 0.75\n",
    "- [x]  Выводы написаны"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}